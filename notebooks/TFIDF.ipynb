{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from easydict import EasyDict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "# split copyright\n",
    "# count vectorize\n",
    "# delete common words\n",
    "# delete anything that doesn't start with ascii characters\n",
    "# select top words to make vocabulary\n",
    "# try different values of C, vector sizes\n",
    "# make cross-val splits\n",
    "# get tf-idf features\n",
    "# run logistic regression\n",
    "# get scores\n",
    "# for best model retrain on all\n",
    "# get test score\n",
    "# save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/anush/veg2vec/samples_1/abstracts_labelled_tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(df):\n",
    "    texts = df.text.values\n",
    "    texts = np.stack([t.split('Â©')[0] for t in texts])\n",
    "    labels = df.relevant.values.astype('int')\n",
    "    inds = df.index.values\n",
    "    assert set(np.unique(inds)) == set(range(len(inds))) \n",
    "    txt_trn, txt_test, y_trn, y_test, inds_trn, inds_test = train_test_split(texts, labels, inds, test_size=0.1, shuffle=True)\n",
    "    splitter = KFold(n_splits=5, shuffle=True)\n",
    "    splits = list(splitter.split(txt_trn))\n",
    "    return EasyDict(\n",
    "        corpus=EasyDict(train=txt_trn, test=txt_test),\n",
    "        labels=EasyDict(train=y_trn, test=y_test),\n",
    "        inds=EasyDict(train=inds_trn, test=inds_test),\n",
    "        splits=splits,\n",
    "        train_test_splits=(np.arange(len(txt_trn)), \n",
    "                           np.arange(len(txt_trn), len(texts)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(trn, val, max_features=None):\n",
    "    cvec = CountVectorizer(stop_words='english',\n",
    "                          strip_accents='unicode',\n",
    "                          max_features=max_features)\n",
    "    cvec.fit(trn)\n",
    "    exclude = ['author', 'abstract', 'copyright', 'journal', 'article']\n",
    "    vocab = cvec.get_feature_names()\n",
    "    vocab = np.stack(\n",
    "        [\n",
    "            v for v in vocab if v[0] in string.ascii_letters\n",
    "        ]\n",
    "    )\n",
    "    vocab = np.stack(\n",
    "        [\n",
    "            v for v in vocab if not any(v.startswith(i) for i in exclude)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    tidvec = TfidfVectorizer(\n",
    "        vocabulary=vocab,\n",
    "    )\n",
    "    x_trn = tidvec.fit_transform(trn)\n",
    "    x_val = tidvec.transform(val)\n",
    "    return x_trn, x_val, cvec, tidvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(corpus, labels, split, max_features=None):\n",
    "    trn_inds, val_inds = split\n",
    "    x_trn, x_val, cvec, tvec = get_features(corpus[trn_inds], corpus[val_inds], max_features)\n",
    "    return EasyDict(\n",
    "        features = EasyDict(train=x_trn, valid=x_val),\n",
    "        labels = EasyDict(train=labels[trn_inds], valid=labels[val_inds]),\n",
    "        cvec=cvec,\n",
    "        tvec=tvec\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic(features, labels, C=None, max_features=None):\n",
    "    model = LogisticRegression(C=C, class_weight='balanced')\n",
    "    model.fit(features.train, labels.train)\n",
    "    \n",
    "    score = f1_score(y_true=labels.valid, y_pred=model.predict(features.valid))\n",
    "    \n",
    "    return EasyDict(\n",
    "        model=model,\n",
    "        score=score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_val(corpus, labels, splits):\n",
    "    scores = {}\n",
    "    names = ['C', 'max_features']\n",
    "    max_features = [3000, 6000, 9000, None]\n",
    "    C = [1, 1.5, 2, 2.5, 3.]\n",
    "\n",
    "    for split in splits:\n",
    "        for mf in max_features:\n",
    "            dt = make_dataset(corpus.train, labels.train, split, mf)\n",
    "            for c in C:\n",
    "                outputs = train_logistic(dt.features, dt.labels, C=c)\n",
    "                scores.setdefault((c, mf), []).append(outputs.score)\n",
    "\n",
    "    scores = {k: np.mean(v) for k, v in scores.items()}\n",
    "\n",
    "    return pd.DataFrame(columns=['C', 'max_features', 'score'], \n",
    "             data=[[c, mf if mf is not None else 'None', v] for (c, mf), v in scores.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "splits = get_splits(df)\n",
    "scores = run_cross_val(splits.corpus, splits.labels, splits.splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>max_features</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.844992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     C max_features     score\n",
       "4  3.0         3000  0.844992"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.nlargest(1, 'score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = scores.nlargest(1, 'score').squeeze()\n",
    "dt = make_dataset(np.concatenate([splits.corpus.train, splits.corpus.test]), \n",
    "                  np.concatenate([splits.labels.train, splits.labels.test]),\n",
    "                  splits.train_test_splits, max_features=best_params.max_features) \n",
    "outputs = train_logistic(dt.features, dt.labels, C=best_params.C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8035714285714285"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/anush/veg2vec/model.pkl', 'wb') as f:\n",
    "    pickle.dump(file=f, obj=(\n",
    "            EasyDict(\n",
    "                splits=splits,\n",
    "                scores=scores,\n",
    "                best_params=best_params,\n",
    "                train_test_dataset=dt,\n",
    "                outputs=outputs\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/anush/veg2vec/model.pkl', 'rb') as f:\n",
    "    p = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=3.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.outputs.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37venv",
   "language": "python",
   "name": "py37venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
